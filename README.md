# WES_pipeline

Kornelia Gladysz

It's important to note that most filepaths in the scripts will need to be edited. Look for "/your_path" and check the directory of the error out output files in the PBS header before running. 

The scripts assume you're using GRCh38. If not, you'll need to make additional edits where the build is referenced. 

Steps 5B and 5C are not essential for variant calling but are useful for QC. 

## Step 1: Generate unmapped BAM from FASTQ

Generate an unmapped BAM from FASTQ
Convert FASTQ to uBAM and add read group information using FastqToSam.
For unmapped BAM files you can assign read groups which let you specify example what platform was used (Sanger, Illumina, Roche 454, IonTorrent, etc) and the sample name (e.g. Cancer vs Healthy). (https://blastedbio.blogspot.co.uk/2011/10/fastq-must-die-long-live-sambam.html )

We are analyzing pair-end whole exome sequencing data. The crucial part for the first step of the pipeline is the file names, which in this case is:
HLMM2BBXX_15607628_S19_L003_R1_001.fastq.gz
HLMM2BBXX_15607628_S19_L003_R2_001.fastq.gz

Where reads for a forward primer are in the R1 file and reads start with the reverse primer should be in R2 file. 

From the file name, we can get information to assign read groups, such as sample name (S19), lane number (3) and platform unit (HLMM2BBXX.3).
Used a “for” loop, which takes each R1 fastq file, combines it with the R2 pair and gives a proper read groups to each pair of fastq files. In this way we don’t have to do it for each fastq pair separately.  




## Step 2: Mark adapter sequences using MarkIlluminaAdapters

MarkIlluminaAdapters adds the XT tag to a read record to mark the 5' start position of the specified adapter sequence and produces a metrics file. Some of the marked adapters come from concatenated adapters that randomly arise from the primordial soup that is a PCR reaction. Others represent read-through to 3' adapter ends of reads and arise from insert sizes that are shorter than the read length.
(https://software.broadinstitute.org/gatk/documentation/article.php?id=6483 ) 

Again we don’t want to mark adapters for each sample separately, so this “for” loop will take each uBAM file created in previous step and run MarkIlluminaAdapters for it. 






## Step3: Align reads with BWA-MEM and merge with uBAM using MergeBamAlignment

This step actually pipes three processes, performed by three different tools. The three tools we use are SamToFastq, BWA-MEM and MergeBamAlignment. 


## Step3a: Convert BAM to FASTQ and discount adapter sequences using SamToFastq

We use additional options to effectively remove previously marked adapter sequences, in this example marked with an XT tag. By specifying CLIPPING_ATTRIBUTE=XT and CLIPPING_ACTION=2.
For our paired reads example file we set SamToFastq's INTERLEAVE to true. During the conversion to FASTQ format, the query name of the reads in a pair are marked with /1 or /2 and paired reads are retained in the same FASTQ file.

Another “for” loop to avoid repeating everything separately for each file.

## Step3b: Align reads and flag secondary hits using BWA-MEM

BWA alignment requires an indexed reference genome file. If you are downloading the reference from the web site, which I mentioned, you will find all the indexes there, so you can just download them and don’t worry. 

To index the human genome for BWA, we apply BWA's index function on the reference genome file:

`/apps/software/gcc-6.2.0/bwa/0.7.15/bwa index -a bwtsw /your_path/path_to_reference/reference.fa.gz
`

We need five index files with the extensions amb, ann, bwt, pac and sa. After you have reference and index files, you can run next “for” loop to align reads with BWA MEM.





## Step3c: Restore altered data and apply & adjust meta information using MergeBamAlignment

The tool merges defined information from the unmapped BAM (uBAM, step 1) with that of the aligned BAM (step 3) to conserve read data, e.g. original read information and base quality scores. The tool also generates additional meta information based on the information generated by the aligner, which may alter aligner-generated designations, e.g. mate information and secondary alignment flags.

Before running this part we need to have dictionary for the reference genome, if you didn’t download it, this command will help you to do that:

`java -jar /apps/software/java-jdk-1.8.0_92/picard/2.8.1/picard.jar CreateSequenceDictionary REFERENCE=/your_path/refrence/refrence.fa.gz OUTPUT=/group/godley-lab/reference/reference.dict
`


The dictionary file is formatted like a SAM header, describing the contents of your reference FASTA file. After you got it, you can run the next step.




## Step 4: Mark duplicates

Since we do not need sorting, because our BAM files are already sorted, we directly go to mark duplicated. To check if your files are sorted:

`samtools view –H bam_file_name.bam
`

SO:coordinate means the file is sorted by coordinate. If it would be not sorted then you will see: SO:unsorted.

About marking duplicates, the idea here is that during the sequencing process, the same DNA fragments may be sequenced several times. The resulting duplicate reads are not informative and should not be counted as additional evidence for or against a putative variant. 
The duplicate marking process (sometimes called **dedupping** in bioinformatics slang) does not remove the reads, but identifies them as duplicates by adding a flag in the read's SAM record. Most GATK tools will then ignore these duplicate reads by default, through the internal application of a read filter.


## Step 5: Recalibrate Bases

Variant calling algorithms rely heavily on the quality scores assigned to the individual base calls in each sequence read. These scores are per-base estimates of error emitted by the sequencing machines. The scores produced by the machines are subject to various sources of systematic technical error, leading to over- or under-estimated base quality scores in the data. 
Base quality score recalibration (BQSR) is a process in which we apply machine learning to model these errors empirically and adjust the quality scores accordingly. This allows us to get more accurate base qualities, which in turn improves the accuracy of our variant calls.
The base recalibration process involves two key steps: first the program builds a model of covariation based on the data and a set of known variants, then it adjusts the base quality scores in the data based on the model. The known variants are used to mask out bases at sites of real (expected) variation, to avoid counting real variants as errors. Outside of the masked sites, every mismatch is counted as an error. The rest is mostly accounting. 

To get a set of known variants, you will need to download dbSNP (The Single Nucleotide Polymorphism Database)
It is important to use the same version of reference in known variants (here we are using all the time version 38).


## Step 5a: Analyze patterns of covariation in the sequence dataset

This creates a GATK Report file called recal_data.table containing several tables. These tables contain the covariation data that will be used in a later step to recalibrate the base qualities of your sequence data.

****Important note: you can use: `-U ALLOW_SEQ_DICT_INCOMPATIBILITY` which helps to avoid the error about incompatibility of contigs in reference and dbSNP. But I would try to avoid it, if possible.


## Step 5b: Do a second pass to analyze covariation remaining after recalibration

****** You do not need step 5b and 5c to create your .bam file!!! You can run 5d right away after finishing step 5a. 5b and 5c is showing you the improvement – quality control ********

This step is needed to create the quality plots before and after the base recalibration.


## Step 5c: Generate before/after plots

Comparing the before and after plots allows you to check the effect of the base recalibration process before you actually apply the recalibration to your sequence data.

For this purpose, I had to install few new packages in R, since otherwise there were few errors occurring. Install: gplots. 
And run for loop, which takes only few minutes, so you don’t really need to send it on cluster.

## Step 5d: Apply the recalibration to your sequence data

This creates a .bam file containing all the original reads, but now with exquisitely accurate base substitution, insertion and deletion quality scores. By default, the original quality scores are discarded in order to keep the file size down. 
Notice how this step uses a very simple tool, PrintReads, to apply the recalibration. What’s happening here is that we are loading in the original sequence data, having the GATK engine recalibrate the base qualities on-the-fly thanks to the -BQSR flag (as explained earlier), and just using PrintReads to write out the resulting data to the new file.


## Step 6: Variant Discovery

Calling variants is the last step (next you can do any kind of filtering).
After that step we are getting the .vcf files with all the variants.
Here I include example script, which you will have to modify for your needs. In this case I didn’t use for loop, just I manually choose, what do I want to include.

To zip vcf file for analysis use tabix (which is part of samtools 1.5 on gardner):

`bgzip -c file.vcf > file.vcf.gz`

`tabix -p vcf file.vcf.gz
`

First part will zip your file and the second command will create a index file for your zipped vcf. As a next thing, you can open .vcf files in BasePlayer and by VariantManager find the variants you are interested in. 






























